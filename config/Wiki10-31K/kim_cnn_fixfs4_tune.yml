# data
data_dir: data/Wiki10-31K
data_name: Wiki10-31K
min_vocab_freq: 1
max_seq_length: 500

# train
seed: 1337
epochs: 50
batch_size: 64
optimizer: adam
learning_rate: ['choice', [0.0001, 0.0003, 0.001, 0.003, 0.01]]
weight_decay: 0
patience: 5
shuffle: true

# eval
eval_batch_size: 256
monitor_metrics: [P@1, P@3, P@5]
val_metric: P@3

# model
model_name: KimCNN
num_filter_per_size: ['choice', [96, 192, 384, 768]] # filter channels
filter_sizes: [4]
dropout: ['choice', [0.2, 0.4, 0.6, 0.8]]
init_weight: kaiming_uniform
activation: relu

# pretrained vocab / embeddings
embed_file: glove.6B.300d
vocab_file: null

# hyperparamter search
search_alg: optuna
embed_cache_dir: .vector_cache

# other parameters specified in main.py::get_args
config: config/Wiki10-31K/kim_cnn_fixfs4_tune.yml
cpu: false
data_workers: 4
display_iter: 100
eval: false
fixed_length: false
label_file: null
load_checkpoint: null
metrics_thresholds: [0.5]
momentum: 0.9
predict_out_path: null
result_dir: runs
silent: true
test_path: data/Wiki10-31K/test.txt
train_path: data/Wiki10-31K/train.txt
val_path: null
val_size: 0.2
